{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true,
      "mount_file_id": "1_Uvigokhh9Z8hq6TiY33zkpgmkFjPLYN",
      "authorship_tag": "ABX9TyPRQHzdsHO0KsGKiLYC25cM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushi-gajendra/SkinShots_AI_powered_skincare_platform/blob/tensorflow-model/Model_for_SkinShots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model for our SkinShots Website**\n",
        "We are creating a Multiclass Classification Neural Network Model for our SkinShots project.\n",
        "\n",
        "### **Key Concepts & Steps Covered:**\n",
        "\n",
        "1. **Understanding the Data**\n",
        "\n",
        "* We are starting with our **Skin** dataset.\n",
        "* We define **features and labels**.\n",
        "* We inspect it (shape, number of classes, etc.).\n",
        "\n",
        "2. **Preprocessing**\n",
        "\n",
        "* We **scale/normalize** data, if needed.\n",
        "* We possibly **encode categorical labels**.\n",
        "* We split it into **training, validation, test** sets.\n",
        "\n",
        "3. **Building the Model**\n",
        "\n",
        "* We define a neural network architecture using **tf.keras.Sequential** .\n",
        "* Layers usually include **input** layer (or specifying input shape), **hidden** layers (Dense, with activation like ReLU), and **output** layer (with activation appropriate for classification, softmax for multi-class).\n",
        "\n",
        "4. **Compile the Model**\n",
        "\n",
        "* We specify **loss** function (e.g. sparse_categorical_crossentropy or binary_crossentropy depending on setup).\n",
        "* Specify **optimizer** (e.g. Adam).\n",
        "* **Metrics** (like accuracy) to monitor.\n",
        "\n",
        "5. **Training**\n",
        "\n",
        "* **Fit** the model on **training data.**\n",
        "* We use **validation data** to see how well the model is generalizing.\n",
        "* We use **callbacks** or track **history.**\n",
        "\n",
        "6. **Evaluation**\n",
        "\n",
        "* Evaluate on **test data** to see final performance.\n",
        "* Check metrics, possibly **confusion matrix** or other **classification metrics**.\n",
        "\n",
        "7. **Prediction**\n",
        "\n",
        "* Use the model to **predict new/unseen data.**\n",
        "* We possibly inspect **how certain it is** (softmax probabilities, etc.)."
      ],
      "metadata": {
        "id": "EfuLkZOXIDU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 1. **Understanding the Data**\n",
        "\n",
        "* The data is in our **Skin** Directory - with 5 subfolders representing **5 classes**\n",
        "* The 5 classes are - **Acne, Blackheads, Dark Spots, Pores, Wrinkles**\n"
      ],
      "metadata": {
        "id": "UMDGIaYfUR_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **Preprocessing the Data**\n",
        "\n",
        "* We will first split the data into - **Training, Validation** and **Test** sets\n",
        "\n"
      ],
      "metadata": {
        "id": "8PM3tfFrMfmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Getting the Training data (70%)\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Skin\",\n",
        "    image_size=(256,256),\n",
        "    batch_size=32,\n",
        "    label_mode=\"categorical\",\n",
        "    validation_split=0.3,\n",
        "    subset=\"training\",\n",
        "    seed=42)\n",
        "\n",
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS4SaCCNb8KP",
        "outputId": "c85bfc21-5809-4507-c3fa-e0669233dfbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8182 files belonging to 5 classes.\n",
            "Using 5728 files for training.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why it shows `None` instead of our batch size?\n",
        "\n",
        "The dataset object (tf.data.Dataset) is abstract, it doesn‚Äôt know the batch size at the metadata level.\n",
        "\n",
        "None means ‚Äúvariable dimension‚Äù ‚Äî TensorFlow is leaving it flexible, because the last batch might not always be exactly 32.\n",
        "\n",
        "Example: if our dataset had 5728 images, dividing by 32 gives 179.0 batches exactly.\n",
        "\n",
        "If it were 5730 images, the last batch would only have 2 images.\n",
        "\n",
        "So TensorFlow shows None to indicate ‚Äúbatch dimension depends on runtime‚Äù."
      ],
      "metadata": {
        "id": "SPF8loLreobN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Validation and Testing data\n",
        "\n",
        "temp_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/Skin\",\n",
        "    image_size=(256,256),\n",
        "    batch_size=32,\n",
        "    label_mode=\"categorical\",\n",
        "    validation_split=0.3,\n",
        "    subset=\"validation\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Splitting temp (30%) data into Validation (15%) and Testing (15%) data\n",
        "\n",
        "total_batches = tf.data.experimental.cardinality(temp_ds).numpy()\n",
        "temp_batches = int(0.5 * total_batches)\n",
        "val_ds = temp_ds.take(temp_batches)\n",
        "test_ds = temp_ds.skip(temp_batches)\n",
        "\n",
        "val_ds, test_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLzekzVDjpfV",
        "outputId": "9c646c6a-3262-412d-83b0-6e5bf7c9d1f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8182 files belonging to 5 classes.\n",
            "Using 2454 files for validation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<_TakeDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))>,\n",
              " <_SkipDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 5), dtype=tf.float32, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üëâüèª **Theory:**\n",
        "\n",
        "* When you use **image_dataset_from_directory()** (or other dataset creation functions), you get a **tf.data.Dataset** object.\n",
        "\n",
        "* A tf.data.Dataset is an **iterator-like pipeline**, not a static list. So you can‚Äôt just call len(dataset).\n",
        "\n",
        "* To know how many elements (batches) are inside a dataset, TensorFlow provides the **tf.data.experimental.cardinality() function**.\n",
        "\n",
        "* **Cardinality** means ‚Äúnumber of elements.‚Äù\n",
        "\n",
        "* Here, **each element** = **1 batch** (not individual images).\n",
        "\n",
        "* The result is a **tf.Tensor** containing the **count**.\n",
        "\n",
        "* **.numpy()** converts that tensor to a Python integer.\n",
        "\n",
        "* üîπ **Syntax:** `tf.data.experimental.cardinality(dataset)`\n",
        "\n",
        "\n",
        "* üîπ **Parameters:** dataset: a tf.data.Dataset object.\n",
        "\n",
        "* üîπ **Returns:** A scalar tf.Tensor with the number of elements (batches).\n",
        "\n",
        "\n",
        "\n",
        "* **take()** and **skip()** are functional dataset transformations ‚Üí they let us **slice datasets** into **non-overlapping subsets**.\n",
        "* üîπ **Syntax:**\n",
        "\n",
        "* * ` dataset.take(n) ‚Üí first n batches` : this becomes the validation set\n",
        "\n",
        "\n",
        "* * ` dataset.skip(n) ‚Üí everything after the first n batches` : this becomes the testing set"
      ],
      "metadata": {
        "id": "2sMd0FjqG8rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Further understanding the data**\n",
        "\n"
      ],
      "metadata": {
        "id": "gVVw35Q_rRQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a. ***Features & Labels***\n",
        "\n",
        "Each **dataset** element is a **tuple**: (images, labels)\n",
        "\n",
        "**Features** (X): The **input data** we feed into our model.\n",
        "\n",
        "* In our skin project ‚Üí the images (pixels).\n",
        "\n",
        "* Data type ‚Üí tf.float32 (pixel values, usually scaled 0‚Äì255 or normalized to 0‚Äì1)\n",
        "\n",
        "\n",
        "**Labels** (y): The **target/output** the model is trying to predict.\n",
        "\n",
        "* In our project ‚Üí the skin condition **class** (Acne, Blackheads, Dark Spots, Pores, Wrinkles).\n",
        "\n",
        "* Because we used label_mode = \"categorical\", **labels are one-hot encoded vectors**.\n"
      ],
      "metadata": {
        "id": "zExjMOndKcge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. ***Inspect the dataset***\n",
        "\n",
        "* Check number of classes and names.\n",
        "* Check the shapes of one batch.\n"
      ],
      "metadata": {
        "id": "dLkjnwoNdT5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_ds.class_names\n",
        "print(\"Class Name:\", class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjxzodBDyiIO",
        "outputId": "599b4396-a477-4e39-d63f-2697940b55e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Name: ['acne', 'blackheades', 'dark spots', 'pores', 'wrinkles']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of classes:\", len(class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "793QlnkkzIIz",
        "outputId": "1ce1d4da-2ff9-46cd-bce4-3b5d8dcbe17b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_ds.take(1):\n",
        "  print(\"Image batch shape:\", images.shape)\n",
        "  print(\"Label batch shape:\", labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U-Fb5SSzi8t",
        "outputId": "322e0ce9-4df2-4748-a565-5ddebb5fdaba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: (32, 256, 256, 3)\n",
            "Label batch shape: (32, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Features** (X):\n",
        "\n",
        "* **Shape:** `(batch_size, height, width, channels)`\n",
        "\n",
        "* (32, 256, 256, 3) ‚Üí 32 RGB images, each 256√ó256.\n",
        "\n",
        "**Labels** (y):\n",
        "\n",
        "* **Shape**: `(batch_size, num_classes)`\n",
        "\n",
        "* (32, 5) ‚Üí 32 labels, each a vector like [0,0,1,0,0] (meaning \"Dark Spots\")."
      ],
      "metadata": {
        "id": "diltUx8ceLhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **Building the model**"
      ],
      "metadata": {
        "id": "ijpTKGEEfA4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Comipling the model**\n"
      ],
      "metadata": {
        "id": "0_GyxLEDrarY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **Training the model**"
      ],
      "metadata": {
        "id": "VVM5WXb3rco8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model - CNN model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    # 1st convulation + pooling\n",
        "    tf.keras.layers.Conv2D(filters=30, kernel_size=(3,3), activation=\"relu\", input_shape=(256,256,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # 2nd convulation + pooling\n",
        "    tf.keras.layers.Conv2D(filters=60, kernel_size=(3,3), activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    # Flatten feature maps ‚Üí Dense layers\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(120, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(5, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss = tf.keras.losses.CategoricalCrossentropy,\n",
        "              optimizer = tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Training/fitting the model\n",
        "model.fit(train_ds, validation_data= val_ds, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rdJonUJNfho2",
        "outputId": "a119f041-d03c-4b1a-ae53-dd11bcf53463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m 67/179\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m9:21\u001b[0m 5s/step - accuracy: 0.3676 - loss: 855.0195"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why we choose a CNN model for images**\n",
        "\n",
        "* **Images** are not just random numbers ‚Äî they have **spatial structure**(neighboring pixels form edges, textures, patterns).\n",
        "\n",
        "* A normal **Dense** neural net:\n",
        "* * Would need millions of parameters to handle a 256√ó256√ó3 image.\n",
        "* * Ignores pixel positions (treats them all the same).\n",
        "\n",
        "* **CNNs** are **designed to handle images** because they:\n",
        "\n",
        "* * Look at small regions at a time (**local patterns**).\n",
        "\n",
        "* * Reuse filters across the whole image (**fewer parameters**).\n",
        "\n",
        "* * Build up from **edges ‚Üí textures ‚Üí shapes ‚Üí objects**."
      ],
      "metadata": {
        "id": "AebBeQHw11nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why we use each part of a CNN model**\n",
        "\n",
        "1. **Input Layer**\n",
        "\n",
        "* Why we need it:\n",
        "* * The image comes in as (256, 256, 3) (height, width, RGB channels).\n",
        "\n",
        "* Why not flatten immediately?\n",
        "* * If we flatten right away, we lose the spatial relationships (neighboring pixels that form edges/spots).\n",
        "\n",
        "2. **Convolutional Layers (Conv2D)**\n",
        "\n",
        "* Why:\n",
        "* * Instead of learning weights for every pixel separately, Conv2D uses filters/kernels that slide across the image, learning local patterns (edges, pores, spots).\n",
        "\n",
        "* * Looks at small patches of the image (like 3√ó3 pixels).\n",
        "\n",
        "* Why not Dense from the start?\n",
        "* * Dense layers treat each pixel independently ‚Üí too many parameters (millions!) and no sense of location.\n",
        "\n",
        "* Benefit: Much fewer parameters, captures spatial features.\n",
        "\n",
        "3. **Activation Function (ReLU)**\n",
        "\n",
        "* Why:\n",
        "* * Without non-linearity, the network is basically just doing linear transformations.\n",
        "\n",
        "* * ReLU (f(x)=max(0,x)) is simple and prevents vanishing gradients (a common training problem).\n",
        "\n",
        "* * Just turns all negative numbers into 0. This helps the network learn faster and not get stuck.\n",
        "\n",
        "* Why not Sigmoid/Tanh?\n",
        "* * They squish values into small ranges, making deep networks hard to train.\n",
        "* * ReLU is faster and works better for images.\n",
        "\n",
        "4. **Pooling Layers (MaxPooling2D)**\n",
        "\n",
        "* Why:\n",
        "* * Images are huge, and we don‚Äôt need every pixel.\n",
        "* * Pooling reduces size while keeping the most important info (like ‚Äúwas there a spot in this region?‚Äù).\n",
        "\n",
        "* * Shrinks the image while keeping the most important part.\n",
        "\n",
        "* * Example: instead of remembering every pixel of a spot, it just remembers ‚Äúthere was a strong spot here‚Äù.\n",
        "\n",
        "* Why MaxPooling and not AveragePooling?\n",
        "\n",
        "* * MaxPooling keeps the strongest signal (the most activated feature). AveragePooling can blur/lose sharp features.\n",
        "\n",
        "* Benefit: Smaller feature maps ‚Üí fewer computations, more robust to small shifts (if a spot moves slightly, the model still detects it).\n",
        "\n",
        "5. **Flatten**\n",
        "\n",
        "* Why:\n",
        "* * After convolution + pooling, we have a 3D feature map (height √ó width √ó channels).\n",
        "\n",
        "* * Flatten converts it into a 1D vector so we can feed it into Dense layers.\n",
        "\n",
        "* Why not keep it 3D? Dense layers only work with 1D input.\n",
        "\n",
        "6. **Dense (Fully Connected) Layers**\n",
        "\n",
        "* Why:\n",
        "* * These layers combine all extracted features and ‚Äúdecide‚Äù what class the image belongs to.\n",
        "\n",
        "* * Think of them as the classifier that sits on top of the feature extractor.\n",
        "\n",
        "* * Think of this part as the decision maker: ‚Äúbased on these features, is it acne, pores, or wrinkles?‚Äù\n",
        "\n",
        "* Why not only Conv layers?\n",
        "* * Conv layers are great at feature extraction, but Dense layers are good at combining them for final decisions.\n",
        "\n",
        "7. **Output Layer (Dense with Softmax)**\n",
        "\n",
        "* Why Softmax?\n",
        "* * Converts the raw numbers (logits) into probabilities across your 5 classes.\n",
        "\n",
        "* * Ensures they add up to 1, so you can interpret it as: ‚Äú70% Acne, 20% Dark Spot, 10% Pores‚Äù.\n",
        "\n",
        "* Why not Sigmoid?\n",
        "* * Sigmoid is for binary classification (yes/no).\n",
        "* * For multiclass, we need Softmax.\n",
        "\n",
        "**Dropout (optional, but often used)**\n",
        "\n",
        "Why: Prevents overfitting by randomly turning off some neurons during training.\n",
        "\n",
        "This forces the model to learn robust patterns, not memorize the training set.\n",
        "\n",
        "Why not always? Too much dropout = underfitting.\n",
        "\n",
        "\n",
        "‚úÖ **So in simple terms:**\n",
        "\n",
        "* Conv2D = feature finder (edges, spots).\n",
        "\n",
        "* ReLU = makes learning faster.\n",
        "\n",
        "* Pooling = keeps only important stuff.\n",
        "\n",
        "* Flatten + Dense = final decision making.\n",
        "\n",
        "* Dropout = avoids overfitting.\n",
        "\n",
        "* Softmax = gives probabilities for each class.\n",
        "\n",
        "CNN is chosen because **it‚Äôs built for image data.**\n",
        "\n",
        "Inside CNN: each part **(Conv ‚Üí ReLU ‚Üí Pool ‚Üí Dense ‚Üí Softmax)** plays a role in extracting features, simplifying them, and finally making a classification."
      ],
      "metadata": {
        "id": "KfTGjCOc2c-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. **Evaluating the model**"
      ],
      "metadata": {
        "id": "YTfed05T2ZII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "736ny8VitD2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "c4ppKZFYsF_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}